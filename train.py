# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.
"""

"""
Fine-tuning Meta's SeamlessM4T-medium model using LoRA (Low-Rank Adaptation)
for efficient training on Serbian-to-English speech translation task.

Dataset:
    - Ju≈æne Vesti (train, validation, test splits)
"""

import os
import torch
import librosa
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Any, Dict, List, Union

from datasets import Dataset
from transformers import (
    SeamlessM4TModel,
    SeamlessM4TProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
import evaluate


# ============================================================================
# CONFIGURATION
# ============================================================================

# Model configuration
MODEL_NAME = "facebook/hf-seamless-m4t-medium"

# Audio sampling rate
SAMPLE_RATE = 16000

# Output directories
OUTPUT_DIR = "./seamless-lora-juzneves"
FINAL_MODEL_DIR = "./seamless-lora-juzneves-final"

# Data paths for Ju≈æne Vesti dataset
DATA_PATHS = {
    # TSV files (format: audio_file\ttranslation)
    "train_txt": "data/JV_train.txt",
    "val_txt": "data/JV_val.txt",
    "test_txt": "data/reference-JV-test.txt",

    # Audio directories
    "train_audio": "data/JuzneVestiTrain",
    "val_audio": "data/JuzneVestiVal",
    "test_audio": "data/JuzneVestiTest",
}

# LoRA configuration
LORA_CONFIG = {
    "r": 16,
    "lora_alpha": 32,
    "target_modules": [
        "q_proj",
        "v_proj",
        "k_proj",
        "out_proj",
        "fc1",
        "fc2"
    ],
    "lora_dropout": 0.05,
    "bias": "none"
}

# Training configuration
TRAINING_CONFIG = {
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-5,
    "warmup_ratio": 0.1,
    "num_train_epochs": 3,
    "fp16": True,
    "gradient_checkpointing": True,
}


# ============================================================================
# DATA LOADING FUNCTIONS
# ============================================================================

def find_audio_base_path(base_folder: str) -> str:
    """
    Find the actual folder containing audio files.

    Handles different extraction structures from zip files.

    Args:
        base_folder: Base directory to search

    Returns:
        Path to directory containing .wav files
    """
    possible_paths = [
        base_folder,
        os.path.join(base_folder, os.path.basename(base_folder)),
        os.path.join(base_folder, os.path.basename(base_folder), "JuzneVesti-SR"),
    ]

    for path in possible_paths:
        if os.path.exists(path):
            files = os.listdir(path)
            if any(f.endswith('.wav') for f in files):
                print(f"  ‚úÖ Found audio files in: {path}")
                return path

    return base_folder


def load_juzneves_data(txt_file: str, audio_folder: str) -> Dataset:
    """
    Load Ju≈æne Vesti dataset from TSV file.

    File Format:
        - TSV (tab-separated) with columns: audio_file, translation
        - audio_file: filename only (e.g., "file.wav")
        - translation: English translation text

    Args:
        txt_file: Path to TSV file
        audio_folder: Base directory containing audio files

    Returns:
        HuggingFace Dataset with audio paths and translations
    """
    # Load TSV file
    df = pd.read_csv(txt_file, sep='\t')

    # Find actual audio directory
    actual_audio_path = find_audio_base_path(audio_folder)

    # Build audio paths and collect translations
    audio_paths = []
    translations = []
    audio_filenames = []
    missing_count = 0

    for idx, row in df.iterrows():
        audio_path = os.path.join(actual_audio_path, row['audio_file'])

        if os.path.exists(audio_path):
            audio_paths.append(audio_path)
            translations.append(row['translation'])
            audio_filenames.append(row['audio_file'])
        else:
            missing_count += 1
            if missing_count <= 5:
                print(f"  ‚ö†Ô∏è  File not found: {row['audio_file']}")

    if missing_count > 5:
        print(f"  ‚ö†Ô∏è  ... and {missing_count - 5} more files missing")

    print(f"  ‚úÖ Loaded {len(audio_paths)}/{len(df)} audio files")

    return Dataset.from_dict({
        "audio": audio_paths,
        "translation": translations,
        "audio_file": audio_filenames
    })


# ============================================================================
# DATA PREPROCESSING
# ============================================================================

def preprocess_function(examples, processor):
    """
    Preprocess audio and text for SeamlessM4T model.

    Args:
        examples: Batch of examples from dataset
        processor: SeamlessM4T processor

    Returns:
        Dictionary with processed features and labels
    """
    audio_arrays = []

    # Load audio files using librosa
    for audio_path in examples["audio"]:
        try:
            audio_array, _ = librosa.load(audio_path, sr=SAMPLE_RATE)
            audio_arrays.append(audio_array)
        except Exception as e:
            print(f"‚ùå Error loading {audio_path}: {e}")
            audio_arrays.append(np.zeros(SAMPLE_RATE))

    # Process audio for model
    inputs = processor(
        audio=audio_arrays,
        sampling_rate=SAMPLE_RATE,
        return_tensors="pt",
        padding=True
    )

    # Process target text (English translation)
    labels = processor(
        text=examples["translation"],
        tgt_lang="eng",
        return_tensors="pt",
        padding=True
    ).input_ids

    # Replace padding token with -100 (ignored in loss)
    labels[labels == processor.tokenizer.pad_token_id] = -100

    return {
        "input_features": inputs.input_features,
        "attention_mask": inputs.attention_mask,
        "labels": labels
    }


# ============================================================================
# DATA COLLATOR
# ============================================================================

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """
    Data collator for SeamlessM4T speech-to-text translation.
    """
    processor: Any

    def __call__(
        self,
        features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        """Process a batch of features."""
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        # Pad input features
        batch = self.processor.feature_extractor.pad(
            input_features,
            return_tensors="pt"
        )

        # Pad labels
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            return_tensors="pt"
        )

        # Replace padding with -100
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1),
            -100
        )

        batch["labels"] = labels
        return batch


# ============================================================================
# EVALUATION METRICS
# ============================================================================

def compute_metrics(pred, processor, wer_metric):
    """
    Compute WER metric with validation.

    Args:
        pred: Prediction output from model
        processor: SeamlessM4T processor
        wer_metric: HuggingFace WER metric

    Returns:
        Dictionary with computed metrics
    """
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    vocab_size = processor.tokenizer.vocab_size
    pad_token_id = processor.tokenizer.pad_token_id

    # Validate prediction token IDs
    if isinstance(pred_ids, np.ndarray):
        invalid_mask = (pred_ids < 0) | (pred_ids >= vocab_size)
        if invalid_mask.any():
            num_invalid = invalid_mask.sum()
            print(f"‚ö†Ô∏è  Found {num_invalid} invalid token IDs")
            pred_ids = pred_ids.copy()
            pred_ids[invalid_mask] = pad_token_id

    # Replace -100 with pad token ID
    label_ids[label_ids == -100] = pad_token_id

    try:
        # Decode predictions and references
        pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
        label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

        # Calculate WER
        wer = wer_metric.compute(predictions=pred_str, references=label_str)

        return {"wer": wer}

    except Exception as e:
        print(f"‚ùå Error in compute_metrics: {e}")
        return {"wer": 1.0}


# ============================================================================
# CUSTOM TRAINER
# ============================================================================

class SeamlessM4TTrainer(Seq2SeqTrainer):
    """
    Custom Trainer for SeamlessM4T with generation fixes.
    """

    def __init__(self, tgt_lang="eng", *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.tgt_lang = tgt_lang

    def prediction_step(
        self,
        model,
        inputs,
        prediction_loss_only,
        ignore_keys=None
    ):
        """
        Override prediction step with complete fix for generation.
        """
        has_labels = "labels" in inputs
        inputs = self._prepare_inputs(inputs)

        # Compute loss
        with torch.no_grad():
            if has_labels:
                with self.compute_loss_context_manager():
                    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
                loss = loss.mean().detach()
            else:
                loss = None

        # Generate predictions
        if self.args.predict_with_generate:
            gen_kwargs = {
                "tgt_lang": self.tgt_lang,
                "generate_speech": False,
                "max_new_tokens": 225,
                "num_beams": 1,
                "num_return_sequences": 1,
                "do_sample": False,
                "return_dict_in_generate": False,
                "output_scores": False,
                "output_attentions": False,
                "output_hidden_states": False,
            }

            generated_output = model.generate(
                input_features=inputs.get("input_features"),
                attention_mask=inputs.get("attention_mask"),
                **gen_kwargs
            )

            # Extract tokens from different output formats
            if hasattr(generated_output, 'sequences'):
                generated_tokens = generated_output.sequences
            elif hasattr(generated_output, 'output_ids'):
                generated_tokens = generated_output.output_ids
            elif isinstance(generated_output, tuple):
                generated_tokens = generated_output[0]
            elif isinstance(generated_output, torch.Tensor):
                generated_tokens = generated_output
            else:
                generated_tokens = generated_output

            # Validate and fix invalid token IDs
            if generated_tokens is not None and isinstance(generated_tokens, torch.Tensor):
                vocab_size = self.tokenizer.vocab_size if hasattr(self.tokenizer, 'vocab_size') else 256000
                pad_token_id = self.tokenizer.pad_token_id if hasattr(self.tokenizer, 'pad_token_id') else 0

                invalid_mask = (generated_tokens < 0) | (generated_tokens >= vocab_size)
                if invalid_mask.any():
                    generated_tokens = generated_tokens.clone()
                    generated_tokens[invalid_mask] = pad_token_id
        else:
            generated_tokens = None

        # Prepare labels
        if has_labels:
            labels = inputs["labels"]
            if generated_tokens is not None:
                max_length = max(labels.shape[-1], generated_tokens.shape[-1])
                if labels.shape[-1] < max_length:
                    labels = self._pad_tensors_to_max_len(labels, max_length)
        else:
            labels = None

        return (loss, generated_tokens, labels)


# ============================================================================
# MAIN TRAINING PIPELINE
# ============================================================================

if __name__ == "__main__":

    print("=" * 70)
    print("Serbian ‚Üí English Speech Translation")
    print("SeamlessM4T-medium Fine-tuning with LoRA")
    print("=" * 70)
    print(f"\nModel: {MODEL_NAME}")
    print(f"Output directory: {OUTPUT_DIR}")
    print(f"Final model directory: {FINAL_MODEL_DIR}")

    # ========================================================================
    # 1. CHECK GPU
    # ========================================================================

    print("\nüîç Checking available resources...")
    print(f"‚úÖ CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    # ========================================================================
    # 2. LOAD DATA
    # ========================================================================

    print("\n" + "=" * 70)
    print("LOADING DATASETS")
    print("=" * 70)

    print("\nüîπ TRAIN SET:")
    train_dataset = load_juzneves_data(
        DATA_PATHS["train_txt"],
        DATA_PATHS["train_audio"]
    )

    print("\nüîπ VALIDATION SET:")
    val_dataset = load_juzneves_data(
        DATA_PATHS["val_txt"],
        DATA_PATHS["val_audio"]
    )

    print("\nüîπ TEST SET:")
    test_dataset = load_juzneves_data(
        DATA_PATHS["test_txt"],
        DATA_PATHS["test_audio"]
    )

    print(f"\nüìä Dataset Summary:")
    print(f"   Training:   {len(train_dataset):,} examples")
    print(f"   Validation: {len(val_dataset):,} examples")
    print(f"   Test:       {len(test_dataset):,} examples")

    # ========================================================================
    # 3. LOAD MODEL AND PROCESSOR
    # ========================================================================

    print("\n" + "=" * 70)
    print("LOADING MODEL")
    print("=" * 70)

    print(f"\nüì¶ Loading SeamlessM4T processor...")
    processor = SeamlessM4TProcessor.from_pretrained(MODEL_NAME)

    print(f"üì¶ Loading SeamlessM4T model in 8-bit...")
    model = SeamlessM4TModel.from_pretrained(
        MODEL_NAME,
        load_in_8bit=True,
        device_map="auto",
        torch_dtype=torch.float16
    )

    print(f"‚úÖ Model loaded!")

    # ========================================================================
    # 4. CONFIGURE LoRA
    # ========================================================================

    print("\n‚öôÔ∏è  Configuring LoRA adapter...")

    lora_config = LoraConfig(
        r=LORA_CONFIG["r"],
        lora_alpha=LORA_CONFIG["lora_alpha"],
        target_modules=LORA_CONFIG["target_modules"],
        lora_dropout=LORA_CONFIG["lora_dropout"],
        bias=LORA_CONFIG["bias"],
        task_type=TaskType.SEQ_2_SEQ_LM
    )

    # Prepare model for LoRA
    model = prepare_model_for_kbit_training(model)

    # Fix BatchNorm/LayerNorm layers
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)) or 'norm' in name:
            module.to(torch.float32)

    # Apply LoRA
    model = get_peft_model(model, lora_config)

    # Print trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nüìä Model Parameters:")
    print(f"   Trainable: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    print(f"   Total:     {total_params:,}")

    # ========================================================================
    # 5. PREPROCESS DATASETS
    # ========================================================================

    print("\n" + "=" * 70)
    print("PREPROCESSING DATASETS")
    print("=" * 70)

    print("\nüîÑ Preprocessing train set...")
    train_dataset = train_dataset.map(
        lambda examples: preprocess_function(examples, processor),
        batched=True,
        batch_size=8,
        remove_columns=["audio", "translation"],
        desc="Preprocessing train"
    )

    print("üîÑ Preprocessing validation set...")
    val_dataset = val_dataset.map(
        lambda examples: preprocess_function(examples, processor),
        batched=True,
        batch_size=8,
        remove_columns=["audio", "translation"],
        desc="Preprocessing validation"
    )

    print("üîÑ Preprocessing test set...")
    test_dataset = test_dataset.map(
        lambda examples: preprocess_function(examples, processor),
        batched=True,
        batch_size=8,
        remove_columns=["audio", "translation"],
        desc="Preprocessing test"
    )

    print("‚úÖ Preprocessing complete!")

    # ========================================================================
    # 6. INITIALIZE DATA COLLATOR
    # ========================================================================

    print("\nüîß Initializing data collator...")
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    print("‚úÖ Data collator ready")

    # ========================================================================
    # 7. LOAD METRICS
    # ========================================================================

    print("\nüìä Loading WER metric...")
    wer_metric = evaluate.load("wer")

    def metrics_fn(pred):
        return compute_metrics(pred, processor, wer_metric)

    print("‚úÖ Metrics ready")

    # ========================================================================
    # 8. CONFIGURE TRAINING
    # ========================================================================

    print("\n" + "=" * 70)
    print("TRAINING CONFIGURATION")
    print("=" * 70)

    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,

        # Batch configuration
        per_device_train_batch_size=TRAINING_CONFIG["per_device_train_batch_size"],
        per_device_eval_batch_size=TRAINING_CONFIG["per_device_eval_batch_size"],
        gradient_accumulation_steps=TRAINING_CONFIG["gradient_accumulation_steps"],

        # Optimization
        learning_rate=TRAINING_CONFIG["learning_rate"],
        warmup_ratio=TRAINING_CONFIG["warmup_ratio"],
        num_train_epochs=TRAINING_CONFIG["num_train_epochs"],

        # Memory optimization
        fp16=TRAINING_CONFIG["fp16"],
        gradient_checkpointing=TRAINING_CONFIG["gradient_checkpointing"],

        # Evaluation and saving
        logging_strategy="epoch",
        save_strategy="epoch",
        eval_strategy="epoch",
        save_total_limit=2,

        # Model selection
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,

        # Generation settings
        predict_with_generate=True,
        generation_max_length=225,

        # Other settings
        optim="adamw_torch",
        report_to=["tensorboard"],
        remove_unused_columns=False,
        push_to_hub=False,
    )

    effective_batch = (
        training_args.per_device_train_batch_size *
        training_args.gradient_accumulation_steps
    )

    print(f"\nüìã Training Settings:")
    print(f"   Epochs:              {training_args.num_train_epochs}")
    print(f"   Batch size per GPU:  {training_args.per_device_train_batch_size}")
    print(f"   Gradient accum.:     {training_args.gradient_accumulation_steps}")
    print(f"   Effective batch:     {effective_batch}")
    print(f"   Learning rate:       {training_args.learning_rate}")
    print(f"   FP16:                {training_args.fp16}")
    print(f"   Gradient checkpoint: {training_args.gradient_checkpointing}")

    # ========================================================================
    # 9. INITIALIZE TRAINER
    # ========================================================================

    print("\nüîß Initializing custom trainer...")
    trainer = SeamlessM4TTrainer(
        tgt_lang="eng",
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=processor.feature_extractor,
        compute_metrics=metrics_fn
    )
    print("‚úÖ Trainer ready")

    # ========================================================================
    # 10. START TRAINING
    # ========================================================================

    print("\n" + "=" * 70)
    print("STARTING TRAINING")
    print("=" * 70)
    print("\nüöÄ Training started...\n")

    # Train the model
    trainer.train()

    print("\n" + "=" * 70)
    print("‚úÖ TRAINING COMPLETE")
    print("=" * 70)

    # ========================================================================
    # 11. SAVE MODEL
    # ========================================================================

    print(f"\nüíæ Saving final model to: {FINAL_MODEL_DIR}")
    model.save_pretrained(FINAL_MODEL_DIR)
    processor.save_pretrained(FINAL_MODEL_DIR)
    print(f"‚úÖ Model and processor saved successfully!")

    # ========================================================================
    # 12. QUICK INFERENCE TEST
    # ========================================================================

    print("\n" + "=" * 70)
    print("QUICK INFERENCE TEST")
    print("=" * 70)

    print("\nüß™ Testing model on a sample from test set...\n")

    # Get original test sample (before preprocessing)
    test_dataset_original = load_juzneves_data(
        DATA_PATHS["test_txt"],
        DATA_PATHS["test_audio"]
    )
    sample = test_dataset_original[0]

    # Load audio
    audio_array, _ = librosa.load(sample["audio"], sr=SAMPLE_RATE)

    # Process audio
    inputs = processor(
        audio=[audio_array],
        sampling_rate=SAMPLE_RATE,
        return_tensors="pt"
    )

    # Move to GPU if available
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}

    # Generate translation
    model.eval()
    with torch.no_grad():
        generated = model.generate(
            input_features=inputs["input_features"],
            attention_mask=inputs.get("attention_mask"),
            tgt_lang="eng",
            generate_speech=False,
            max_new_tokens=225,
            num_beams=1
        )

    # Extract tokens
    if hasattr(generated, 'sequences'):
        generated_tokens = generated.sequences
    elif isinstance(generated, tuple):
        generated_tokens = generated[0]
    else:
        generated_tokens = generated

    # Decode prediction
    prediction = processor.batch_decode(
        generated_tokens,
        skip_special_tokens=True
    )[0]

    # Display results
    print(f"Reference translation:")
    print(f"  {sample['translation'][:150]}...\n")
    print(f"Model prediction:")
    print(f"  {prediction[:150]}...\n")

    # ========================================================================
    # DONE
    # ========================================================================

    print(f"\nModel saved at: {FINAL_MODEL_DIR}")
    print(f"Training logs at: {OUTPUT_DIR}")
    print("\nNext steps:")
    print("  1. Evaluate on test set (compute WER, BLEU, METEOR)")
    print("  2. Compare with baseline model")
