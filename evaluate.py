# -*- coding: utf-8 -*-
"""evaluate.ipynb

Automatically generated by Colab.
"""

"""
Evaluation script for Serbian-to-English speech translation models.

This script computes BLEU, METEOR, and WER metrics by aligning reference
translations with model predictions based on audio file names.

Requirements:
    - sacrebleu
    - jiwer
    - pandas
    - openpyxl
    - nltk

Input files:
    - Reference file: TSV with header (audio_file, translation)
    - Hypothesis file: TSV without header (audio_file, translation)
"""

import sys
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
import nltk
from nltk.translate.meteor_score import meteor_score
import sacrebleu
from jiwer import wer


# ============================================================================
# CONFIGURATION
# ============================================================================

# Input files
REFERENCE_FILE = "data/reference-JV-test.txt"
HYPOTHESIS_FILE = "output/seamless_lora_test_predictions.txt"

# Output prefix for results
OUTPUT_PREFIX = "evaluation_results/seamless_lora"

# Column names
REF_AUDIO_COL = "audio_file"
REF_TRANS_COL = "translation"
HYP_AUDIO_COL = "audio_file"
HYP_TRANS_COL = "translation"

# File format settings
HYPOTHESIS_HAS_HEADER = False  # Set to True if hypothesis file has header row


# ============================================================================
# NLTK SETUP
# ============================================================================

def setup_nltk():
    """Download required NLTK resources."""
    print("Setting up NLTK resources...")

    required_resources = [
        'punkt',
        'wordnet',
        'omw-1.4',
        'punkt_tab'
    ]

    for resource in required_resources:
        try:
            nltk.data.find(f'corpora/{resource}')
        except LookupError:
            try:
                nltk.data.find(f'tokenizers/{resource}')
            except LookupError:
                print(f"  Downloading {resource}...")
                nltk.download(resource, quiet=True)

    print("‚úÖ NLTK setup complete\n")


# ============================================================================
# FILE LOADING AND ALIGNMENT
# ============================================================================

def load_txt_file(file_path: str, has_header: bool = True) -> pd.DataFrame:
    """
    Load TSV file with optional header.

    Args:
        file_path: Path to TSV file
        has_header: Whether file has header row

    Returns:
        DataFrame with columns: audio_file, translation
    """
    if has_header:
        df = pd.read_csv(file_path, sep='\t', encoding='utf-8')
    else:
        # No header - assign default column names
        df = pd.read_csv(file_path, sep='\t', encoding='utf-8', header=None)
        df.columns = ['audio_file', 'translation']

    return df


def align_translations(
    reference_file: str,
    hypothesis_file: str,
    ref_audio_col: str = "audio_file",
    ref_trans_col: str = "translation",
    hyp_audio_col: str = "audio_file",
    hyp_trans_col: str = "translation",
    hyp_has_header: bool = False
) -> pd.DataFrame:
    """
    Load and align reference and hypothesis files by audio filename.

    Args:
        reference_file: Path to reference TSV (with header)
        hypothesis_file: Path to hypothesis TSV
        ref_audio_col: Audio filename column in reference
        ref_trans_col: Translation column in reference
        hyp_audio_col: Audio filename column in hypothesis
        hyp_trans_col: Translation column in hypothesis
        hyp_has_header: Whether hypothesis file has header

    Returns:
        DataFrame with columns: audio_file, reference, hypothesis
    """
    print("=" * 80)
    print("LOADING AND ALIGNING FILES")
    print("=" * 80)

    # Load reference file (with header)
    print(f"\nLoading reference: {reference_file}")
    ref_df = load_txt_file(reference_file, has_header=True)
    print(f"  ‚úì Loaded {len(ref_df)} reference translations")
    print(f"  Columns: {list(ref_df.columns)}")

    # Load hypothesis file
    print(f"\nLoading hypothesis: {hypothesis_file}")
    hyp_df = load_txt_file(hypothesis_file, has_header=hyp_has_header)
    print(f"  ‚úì Loaded {len(hyp_df)} generated translations")
    print(f"  Columns: {list(hyp_df.columns)}")

    # Validate columns exist
    for col, df, name in [
        (ref_audio_col, ref_df, "reference"),
        (ref_trans_col, ref_df, "reference"),
        (hyp_audio_col, hyp_df, "hypothesis"),
        (hyp_trans_col, hyp_df, "hypothesis")
    ]:
        if col not in df.columns:
            raise ValueError(
                f"Column '{col}' not found in {name} file. "
                f"Available: {list(df.columns)}"
            )

    # Normalize audio filenames
    ref_df[ref_audio_col] = ref_df[ref_audio_col].astype(str).str.strip()
    hyp_df[hyp_audio_col] = hyp_df[hyp_audio_col].astype(str).str.strip()

    # Merge by audio filename
    print(f"\nAligning by audio filename...")
    merged_df = ref_df.merge(
        hyp_df[[hyp_audio_col, hyp_trans_col]],
        left_on=ref_audio_col,
        right_on=hyp_audio_col,
        how='inner',
        suffixes=('_ref', '_hyp')
    )

    print(f"  ‚úì Aligned {len(merged_df)} examples")

    # Check for missing files
    missing = len(ref_df) - len(merged_df)
    if missing > 0:
        print(f"  ‚ö†Ô∏è  {missing} audio files from reference not found in hypothesis")

        ref_files = set(ref_df[ref_audio_col])
        hyp_files = set(hyp_df[hyp_audio_col])
        missing_files = ref_files - hyp_files

        if len(missing_files) <= 10:
            print(f"  Missing files:")
            for f in list(missing_files)[:10]:
                print(f"    - {f}")

    # Determine column names (handle suffixes)
    ref_col_name = ref_trans_col
    hyp_col_name = hyp_trans_col

    if f"{ref_trans_col}_ref" in merged_df.columns:
        ref_col_name = f"{ref_trans_col}_ref"
    if f"{hyp_trans_col}_hyp" in merged_df.columns:
        hyp_col_name = f"{hyp_trans_col}_hyp"

    # Create result DataFrame
    result_df = pd.DataFrame({
        'audio_file': merged_df[ref_audio_col] if ref_audio_col in merged_df.columns else merged_df[hyp_audio_col],
        'reference': merged_df[ref_col_name],
        'hypothesis': merged_df[hyp_col_name]
    })

    # Remove NaN values
    nan_count = result_df.isnull().sum().sum()
    if nan_count > 0:
        print(f"  ‚ö†Ô∏è  Found {nan_count} NaN values - removing...")
        result_df = result_df.dropna()
        print(f"  ‚úì {len(result_df)} valid examples remaining")

    # Show sample alignments
    print("\n" + "=" * 80)
    print("SAMPLE ALIGNMENTS:")
    print("=" * 80)
    for idx, row in result_df.head(3).iterrows():
        print(f"\nAudio: {row['audio_file']}")
        print(f"REF:   {row['reference'][:80]}...")
        print(f"HYP:   {row['hypothesis'][:80]}...")

    return result_df


# ============================================================================
# TOKENIZATION
# ============================================================================

def simple_tokenize(text: str) -> List[str]:
    """
    Simple tokenization that works reliably.

    Args:
        text: Input text

    Returns:
        List of tokens
    """
    # Remove punctuation and convert to lowercase
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    # Split and filter empty strings
    tokens = [t for t in text.split() if t.strip()]
    return tokens


# ============================================================================
# METRIC CALCULATION
# ============================================================================

def calculate_bleu(hypotheses: List[str], references: List[str]) -> Dict[str, float]:
    """
    Calculate BLEU score and individual n-gram precisions.

    Args:
        hypotheses: List of hypothesis translations
        references: List of reference translations

    Returns:
        Dictionary with BLEU scores
    """
    refs = [[ref] for ref in references]
    bleu = sacrebleu.corpus_bleu(hypotheses, list(zip(*refs)))

    return {
        "BLEU": round(bleu.score, 2),
        "BLEU-1": round(bleu.precisions[0], 2),
        "BLEU-2": round(bleu.precisions[1], 2),
        "BLEU-3": round(bleu.precisions[2], 2),
        "BLEU-4": round(bleu.precisions[3], 2),
    }


def calculate_meteor(hypotheses: List[str], references: List[str]) -> float:
    """
    Calculate METEOR score with robust tokenization.

    Args:
        hypotheses: List of hypothesis translations
        references: List of reference translations

    Returns:
        Average METEOR score (0-100)
    """
    scores = []
    errors = 0
    error_details = []

    for i, (hyp, ref) in enumerate(zip(hypotheses, references)):
        try:
            hyp_str = str(hyp).strip()
            ref_str = str(ref).strip()

            # Skip empty strings
            if not hyp_str or not ref_str:
                errors += 1
                error_details.append(f"Empty string at position {i}")
                continue

            # Tokenize
            hyp_tokens = simple_tokenize(hyp_str)
            ref_tokens = simple_tokenize(ref_str)

            # Skip if tokenization failed
            if not hyp_tokens or not ref_tokens:
                errors += 1
                error_details.append(f"Empty tokens at position {i}")
                continue

            # Calculate METEOR
            score = meteor_score([ref_tokens], hyp_tokens)
            scores.append(score)

        except Exception as e:
            errors += 1
            if len(error_details) < 5:
                error_details.append(f"Error at position {i}: {str(e)[:50]}")
            continue

    # Report errors
    if errors > 0:
        print(f"    ‚ö†Ô∏è  {errors} examples skipped due to errors")
        if error_details:
            print(f"    Error examples:")
            for detail in error_details[:3]:
                print(f"      - {detail}")

    # Calculate average
    if scores:
        avg_score = sum(scores) / len(scores) * 100
        print(f"    ‚úì Successfully calculated for {len(scores)}/{len(hypotheses)} examples")
        return round(avg_score, 2)
    else:
        print(f"    ‚ö†Ô∏è  No METEOR scores calculated!")
        return 0.0


def calculate_wer_score(hypotheses: List[str], references: List[str]) -> float:
    """
    Calculate Word Error Rate (WER).

    Args:
        hypotheses: List of hypothesis translations
        references: List of reference translations

    Returns:
        WER percentage (0-100, lower is better)
    """
    # Concatenate all hypotheses and references
    hyp_all = " ".join([str(h) for h in hypotheses])
    ref_all = " ".join([str(r) for r in references])

    # Calculate WER
    wer_score = wer(ref_all, hyp_all)
    return round(wer_score * 100, 2)


# ============================================================================
# EVALUATION PIPELINE
# ============================================================================

def evaluate_translation(
    reference_file: str,
    hypothesis_file: str,
    output_prefix: str = "evaluation",
    ref_audio_col: str = "audio_file",
    ref_trans_col: str = "translation",
    hyp_audio_col: str = "audio_file",
    hyp_trans_col: str = "translation",
    hyp_has_header: bool = False
) -> Tuple[Dict[str, float], pd.DataFrame]:
    """
    Complete evaluation pipeline.

    Args:
        reference_file: Path to reference TSV
        hypothesis_file: Path to hypothesis TSV
        output_prefix: Prefix for output files
        ref_audio_col: Audio column in reference
        ref_trans_col: Translation column in reference
        hyp_audio_col: Audio column in hypothesis
        hyp_trans_col: Translation column in hypothesis
        hyp_has_header: Whether hypothesis has header

    Returns:
        Tuple of (metrics dict, aligned DataFrame)
    """
    print("\n" + "=" * 80)
    print("TRANSLATION EVALUATION")
    print("=" * 80)

    # 1. Load and align files
    aligned_df = align_translations(
        reference_file,
        hypothesis_file,
        ref_audio_col,
        ref_trans_col,
        hyp_audio_col,
        hyp_trans_col,
        hyp_has_header
    )

    # 2. Extract and clean translations
    print("\nCleaning data...")
    references = aligned_df['reference'].astype(str).str.strip().tolist()
    hypotheses = aligned_df['hypothesis'].astype(str).str.strip().tolist()

    # Show tokenization sample
    print("\nTokenization sample (first example):")
    if references and hypotheses:
        print(f"  REF original: {references[0][:80]}...")
        print(f"  REF tokens: {simple_tokenize(references[0])[:10]}...")
        print(f"  HYP original: {hypotheses[0][:80]}...")
        print(f"  HYP tokens: {simple_tokenize(hypotheses[0])[:10]}...")

    # 3. Calculate metrics
    print("\n" + "=" * 80)
    print("CALCULATING METRICS")
    print("=" * 80)

    metrics = {}

    # BLEU
    print("\n[1/3] Calculating BLEU...")
    try:
        bleu_metrics = calculate_bleu(hypotheses, references)
        metrics.update(bleu_metrics)
        print(f"  ‚úì BLEU: {bleu_metrics['BLEU']}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error: {e}")

    # METEOR
    print("\n[2/3] Calculating METEOR...")
    try:
        meteor = calculate_meteor(hypotheses, references)
        metrics['METEOR'] = meteor
        print(f"  ‚úì METEOR: {meteor}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error: {e}")

    # WER
    print("\n[3/3] Calculating WER...")
    try:
        wer_score = calculate_wer_score(hypotheses, references)
        metrics['WER'] = wer_score
        print(f"  ‚úì WER: {wer_score}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error: {e}")

    # 4. Display results
    print("\n" + "=" * 80)
    print("FINAL RESULTS")
    print("=" * 80)
    print(f"\nNumber of evaluated examples: {len(aligned_df)}")
    print("\nMetrics:")
    print("-" * 40)
    for metric, value in metrics.items():
        print(f"  {metric:15s}: {value}")

    # Interpretation
    print("\n" + "=" * 80)
    print("INTERPRETATION")
    print("=" * 80)
    print("\nüìä Your results:")
    print("-" * 40)

    if 'BLEU' in metrics:
        status = 'üü¢ Excellent' if metrics['BLEU'] > 40 else 'üü° Good' if metrics['BLEU'] > 30 else 'üî¥ Needs improvement'
        print(f"  BLEU:   {metrics['BLEU']}/100  ‚Üí {status}")

    if 'METEOR' in metrics:
        status = 'üü¢ Excellent' if metrics['METEOR'] > 60 else 'üü° Good' if metrics['METEOR'] > 45 else 'üî¥ Needs improvement'
        print(f"  METEOR: {metrics['METEOR']}/100 ‚Üí {status}")

    if 'WER' in metrics:
        status = 'üü¢ Excellent' if metrics['WER'] < 30 else 'üü° Good' if metrics['WER'] < 50 else 'üî¥ Needs improvement'
        print(f"  WER:    {metrics['WER']}%      ‚Üí {status}")

    print("\nüí° What these numbers mean:")
    print("  ‚Ä¢ BLEU measures n-gram overlap (0-100, higher is better)")
    print("  ‚Ä¢ METEOR measures semantic similarity (0-100, higher is better)")
    print("  ‚Ä¢ WER measures word error rate (0-100, lower is better)")

    # 5. Save results
    print("\n" + "=" * 80)
    print("SAVING RESULTS")
    print("=" * 80)

    # Create output directory
    output_dir = Path(output_prefix).parent
    if output_dir != Path('.'):
        output_dir.mkdir(parents=True, exist_ok=True)

    # Save metrics to JSON
    metrics_file = f"{output_prefix}_metrics.json"
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    print(f"‚úì Metrics: {metrics_file}")

    # Save aligned data
    aligned_file = f"{output_prefix}_aligned.tsv"
    aligned_df.to_csv(aligned_file, sep='\t', index=False, encoding='utf-8')
    print(f"‚úì Aligned data: {aligned_file}")

    # Save with per-example scores
    aligned_df['bleu_score'] = [
        sacrebleu.sentence_bleu(hyp, [ref]).score
        for hyp, ref in zip(hypotheses, references)
    ]
    detailed_file = f"{output_prefix}_detailed.tsv"
    aligned_df.to_csv(detailed_file, sep='\t', index=False, encoding='utf-8')
    print(f"‚úì Detailed results: {detailed_file}")

    print("\n‚úÖ EVALUATION COMPLETE!")

    return metrics, aligned_df


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":

    # Setup NLTK
    setup_nltk()

    # Run evaluation
    metrics, aligned_df = evaluate_translation(
        reference_file=REFERENCE_FILE,
        hypothesis_file=HYPOTHESIS_FILE,
        output_prefix=OUTPUT_PREFIX,
        ref_audio_col=REF_AUDIO_COL,
        ref_trans_col=REF_TRANS_COL,
        hyp_audio_col=HYP_AUDIO_COL,
        hyp_trans_col=HYP_TRANS_COL,
        hyp_has_header=HYPOTHESIS_HAS_HEADER
    )

    print("\n" + "=" * 80)
    print("üìÅ Generated files:")
    print("=" * 80)
    print(f"1. {OUTPUT_PREFIX}_metrics.json    - All metrics in JSON format")
    print(f"2. {OUTPUT_PREFIX}_aligned.tsv     - Aligned references and hypotheses")
    print(f"3. {OUTPUT_PREFIX}_detailed.tsv    - With per-example BLEU scores")
